{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c5071df-7b51-4cd4-ace3-0c28cea37408",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5b94cb6-b99f-4c23-8523-5c5ac8746be2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Fine-tuning LLMs\n",
    " Many LLMs are general purpose models trained on a broad range of data and use cases. This enables them to perform well in a variety of applications, as shown in previous modules. It is not uncommon though to find situations where applying a general purpose model performs unacceptably for specific dataset or use case. This often does not mean that the general purpose model is unusable. Perhaps, with some new data and additional training the model could be improved, or fine-tuned, such that it produces acceptable results for the specific use case.\n",
    " \n",
    " Fine-tuning uses a pre-trained model as a base and continues to train it with a new, task targeted dataset. Conceptually, fine-tuning leverages that which has already been learned by a model and aims to focus its learnings further for a specific task.\n",
    "\n",
    " It is important to recognize that fine-tuning is model training. The training process remains a resource intensive, and time consuming effort. Albeit fine-tuning training time is greatly shortened as a result of having started from a pre-trained model. The model training process can be accelerated through the use of tools like Microsoft's [DeepSpeed](https://github.com/microsoft/DeepSpeed).\n",
    "\n",
    " This notebook will explore how to perform fine-tuning at scale.\n",
    "\n",
    "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
    "1. Prepare a novel dataset\n",
    "1. Fine-tune the `t5-small` model to classify movie reviews.\n",
    "1. Leverage DeepSpeed to enhance training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae2d796d-fe0a-4cd6-be9c-bb69e0bb3d92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1488117105151039>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.databricks.clusterUsageTags.sparkVersion\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTHIS LAB REQUIRES THAT A GPU MACHINE AND RUNTIME IS UTILIZED.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\n",
       "\u001B[0;31mAssertionError\u001B[0m: THIS LAB REQUIRES THAT A GPU MACHINE AND RUNTIME IS UTILIZED."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-1488117105151039>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.databricks.clusterUsageTags.sparkVersion\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTHIS LAB REQUIRES THAT A GPU MACHINE AND RUNTIME IS UTILIZED.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\n\u001B[0;31mAssertionError\u001B[0m: THIS LAB REQUIRES THAT A GPU MACHINE AND RUNTIME IS UTILIZED.",
       "errorSummary": "<span class='ansi-red-fg'>AssertionError</span>: THIS LAB REQUIRES THAT A GPU MACHINE AND RUNTIME IS UTILIZED.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert \"gpu\" in spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\"), \"THIS LAB REQUIRES THAT A GPU MACHINE AND RUNTIME IS UTILIZED.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34382c23-de93-42d6-81ae-b2b2a3938faa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "288991c7-8fa1-466e-9539-c2f21af88ba3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Later sections of this notebook will leverage the DeepSpeed package. DeepSpeed has some additional dependencies that need to be installed in the Databricks environment. The dependencies vary based upon which MLR runtime is being used. The below commands add the necessary libraries accordingly. It is convenient to perform this step at the start of the Notebook to avoid future restarts of the Python kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c92405-bb05-4c8a-8557-dcdb758b60c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-08-17 05:42:54--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcurand-dev-11-7_10.2.10.50-1_amd64.deb\nResolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.195.19.142\nConnecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 41886196 (40M) [application/x-deb]\nSaving to: ‘/tmp/externals/cuda/libcurand-dev-11-7_10.2.10.50-1_amd64.deb’\n\n     0K .......... .......... .......... .......... ..........  0% 64.2M 1s\n    50K .......... .......... .......... .......... ..........  0% 31.7M 1s\n   100K .......... .......... .......... .......... ..........  0% 18.8M 1s\n   150K .......... .......... .......... .......... ..........  0% 98.8M 1s\n   200K .......... .......... .......... .......... ..........  0% 82.5M 1s\n   250K .......... .......... .......... .......... ..........  0%  132M 1s\n   300K .......... .......... .......... .......... ..........  0%  162M 1s\n   350K .......... .......... .......... .......... ..........  0% 82.4M 1s\n   400K .......... .......... .......... .......... ..........  1%  190M 1s\n   450K .......... .......... .......... .......... ..........  1%  128M 1s\n   500K .......... .......... .......... .......... ..........  1% 90.4M 1s\n   550K .......... .......... .......... .......... ..........  1% 90.4M 1s\n   600K .......... .......... .......... .......... ..........  1%  130M 1s\n   650K .......... .......... .......... .......... ..........  1% 45.1M 1s\n   700K .......... .......... .......... .......... ..........  1% 46.8M 1s\n   750K .......... .......... .......... .......... ..........  1%  108M 1s\n   800K .......... .......... .......... .......... ..........  2%  248M 1s\n   850K .......... .......... .......... .......... ..........  2%  250M 1s\n   900K .......... .......... .......... .......... ..........  2%  137M 1s\n   950K .......... .......... .......... .......... ..........  2%  115M 1s\n  1000K .......... .......... .......... .......... ..........  2%  137M 1s\n  1050K .......... .......... .......... .......... ..........  2%  116M 1s\n  1100K .......... .......... .......... .......... ..........  2%  210M 0s\n  1150K .......... .......... .......... .......... ..........  2%  153M 0s\n  1200K .......... .......... .......... .......... ..........  3% 84.9M 0s\n  1250K .......... .......... .......... .......... ..........  3%  151M 0s\n  1300K .......... .......... .......... .......... ..........  3%  151M 0s\n  1350K .......... .......... .......... .......... ..........  3% 79.5M 0s\n  1400K .......... .......... .......... .......... ..........  3%  205M 0s\n  1450K .......... .......... .......... .......... ..........  3%  172M 0s\n  1500K .......... .......... .......... .......... ..........  3%  202M 0s\n  1550K .......... .......... .......... .......... ..........  3%  177M 0s\n  1600K .......... .......... .......... .......... ..........  4% 63.7M 0s\n  1650K .......... .......... .......... .......... ..........  4%  240M 0s\n  1700K .......... .......... .......... .......... ..........  4%  190M 0s\n  1750K .......... .......... .......... .......... ..........  4%  240M 0s\n  1800K .......... .......... .......... .......... ..........  4%  213M 0s\n  1850K .......... .......... .......... .......... ..........  4% 82.5M 0s\n  1900K .......... .......... .......... .......... ..........  4%  141M 0s\n  1950K .......... .......... .......... .......... ..........  4%  195M 0s\n  2000K .......... .......... .......... .......... ..........  5%  145M 0s\n  2050K .......... .......... .......... .......... ..........  5%  251M 0s\n  2100K .......... .......... .......... .......... ..........  5%  168M 0s\n  2150K .......... .......... .......... .......... ..........  5% 78.2M 0s\n  2200K .......... .......... .......... .......... ..........  5%  142M 0s\n  2250K .......... .......... .......... .......... ..........  5%  237M 0s\n  2300K .......... .......... .......... .......... ..........  5% 86.9M 0s\n  2350K .......... .......... .......... .......... ..........  5%  154M 0s\n  2400K .......... .......... .......... .......... ..........  5%  192M 0s\n  2450K .......... .......... .......... .......... ..........  6% 63.5M 0s\n  2500K .......... .......... .......... .......... ..........  6%  220M 0s\n  2550K .......... .......... .......... .......... ..........  6%  141M 0s\n  2600K .......... .......... .......... .......... ..........  6%  106M 0s\n  2650K .......... .......... .......... .......... ..........  6%  247M 0s\n  2700K .......... .......... .......... .......... ..........  6% 67.0M 0s\n  2750K .......... .......... .......... .......... ..........  6%  182M 0s\n  2800K .......... .......... .......... .......... ..........  6%  244M 0s\n  2850K .......... .......... .......... .......... ..........  7% 62.8M 0s\n  2900K .......... .......... .......... .......... ..........  7%  231M 0s\n  2950K .......... .......... .......... .......... ..........  7%  189M 0s\n  3000K .......... .......... .......... .......... ..........  7%  249M 0s\n  3050K .......... .......... .......... .......... ..........  7% 87.0M 0s\n  3100K .......... .......... .......... .......... ..........  7%  139M 0s\n  3150K .......... .......... .......... .......... ..........  7%  178M 0s\n  3200K .......... .......... .......... .......... ..........  7% 74.3M 0s\n  3250K .......... .......... .......... .......... ..........  8%  127M 0s\n  3300K .......... .......... .......... .......... ..........  8%  218M 0s\n  3350K .......... .......... .......... .......... ..........  8% 87.2M 0s\n  3400K .......... .......... .......... .......... ..........  8%  223M 0s\n  3450K .......... .......... .......... .......... ..........  8%  168M 0s\n  3500K .......... .......... .......... .......... ..........  8%  193M 0s\n  3550K .......... .......... .......... .......... ..........  8% 49.7M 0s\n  3600K .......... .......... .......... .......... ..........  8%  160M 0s\n  3650K .......... .......... .......... .......... ..........  9%  254M 0s\n  3700K .......... .......... .......... .......... ..........  9%  249M 0s\n  3750K .......... .......... .......... .......... ..........  9%  227M 0s\n  3800K .......... .......... .......... .......... ..........  9%  208M 0s\n  3850K .......... .......... .......... .......... ..........  9% 63.9M 0s\n  3900K .......... .......... .......... .......... ..........  9%  220M 0s\n  3950K .......... .......... .......... .......... ..........  9%  190M 0s\n  4000K .......... .......... .......... .......... ..........  9% 61.7M 0s\n  4050K .......... .......... .......... .......... .......... 10%  171M 0s\n  4100K .......... .......... .......... .......... .......... 10%  258M 0s\n  4150K .......... .......... .......... .......... .......... 10% 79.5M 0s\n  4200K .......... .......... .......... .......... .......... 10%  224M 0s\n  4250K .......... .......... .......... .......... .......... 10%  107M 0s\n  4300K .......... .......... .......... .......... .......... 10% 58.8M 0s\n  4350K .......... .......... .......... .......... .......... 10%  187M 0s\n  4400K .......... .......... .......... .......... .......... 10%  170M 0s\n  4450K .......... .......... .......... .......... .......... 11%  246M 0s\n  4500K .......... .......... .......... .......... .......... 11% 71.0M 0s\n  4550K .......... .......... .......... .......... .......... 11%  198M 0s\n  4600K .......... .......... .......... .......... .......... 11%  177M 0s\n  4650K .......... .......... .......... .......... .......... 11%  227M 0s\n  4700K .......... .......... .......... .......... .......... 11% 72.4M 0s\n  4750K .......... .......... .......... .......... .......... 11%  145M 0s\n  4800K .......... .......... .......... .......... .......... 11%  173M 0s\n  4850K .......... .......... .......... .......... .......... 11%  168M 0s\n  4900K .......... .......... .......... .......... .......... 12%  113M 0s\n  4950K .......... .......... .......... .......... .......... 12%  205M 0s\n  5000K .......... .......... .......... .......... .......... 12%  202M 0s\n  5050K .......... .......... .......... .......... .......... 12% 66.8M 0s\n  5100K .......... .......... .......... .......... .......... 12%  242M 0s\n  5150K .......... .......... .......... .......... .......... 12%  209M 0s\n  5200K .......... .......... .......... .......... .......... 12% 97.6M 0s\n  5250K .......... .......... .......... .......... .......... 12%  247M 0s\n  5300K .......... .......... .......... .......... .......... 13%  253M 0s\n  5350K .......... .......... .......... .......... .......... 13%  222M 0s\n  5400K .......... .......... .......... .......... .......... 13% 56.0M 0s\n  5450K .......... .......... .......... .......... .......... 13%  244M 0s\n  5500K .......... .......... .......... .......... .......... 13%  246M 0s\n  5550K .......... .......... .......... .......... .......... 13%  204M 0s\n  5600K .......... .......... .......... .......... .......... 13% 90.2M 0s\n  5650K .......... .......... .......... .......... .......... 13%  180M 0s\n  5700K .......... .......... .......... .......... .......... 14%  242M 0s\n  5750K .......... .......... .......... .......... .......... 14% 95.5M 0s\n  5800K .......... .......... .......... .......... .......... 14%  112M 0s\n  5850K .......... .......... .......... .......... .......... 14%  199M 0s\n  5900K .......... .......... .......... .......... .......... 14%  191M 0s\n  5950K .......... .......... .......... .......... .......... 14%  118M 0s\n  6000K .......... .......... .......... .......... .......... 14%  142M 0s\n  6050K .......... .......... .......... .......... .......... 14%  170M 0s\n  6100K .......... .......... .......... .......... .......... 15% 99.6M 0s\n  6150K .......... .......... .......... .......... .......... 15%  229M 0s\n  6200K .......... .......... .......... .......... .......... 15% 74.1M 0s\n  6250K .......... .......... .......... .......... .......... 15%  163M 0s\n  6300K .......... .......... .......... .......... .......... 15%  167M 0s\n  6350K .......... .......... .......... .......... .......... 15%  212M 0s\n  6400K .......... .......... .......... .......... .......... 15% 99.5M 0s\n  6450K .......... .......... .......... .......... .......... 15% 79.4M 0s\n  6500K .......... .......... .......... .......... .......... 16%  184M 0s\n  6550K .......... .......... .......... .......... .......... 16% 66.3M 0s\n  6600K .......... .......... .......... .......... .......... 16%  233M 0s\n  6650K .......... .......... .......... .......... .......... 16%  196M 0s\n  6700K .......... .......... .......... .......... .......... 16%  119M 0s\n  6750K .......... .......... .......... .......... .......... 16%  154M 0s\n  6800K .......... .......... .......... .......... .......... 16%  181M 0s\n  6850K .......... .......... .......... .......... .......... 16%  249M 0s\n  6900K .......... .......... .......... .......... .......... 16%  136M 0s\n  6950K .......... .......... .......... .......... .......... 17% 84.3M 0s\n  7000K .......... .......... .......... .......... .......... 17%  242M 0s\n  7050K .......... .......... .......... .......... .......... 17%  199M 0s\n  7100K .......... .......... .......... .......... .......... 17%  127M 0s\n  7150K .......... .......... .......... .......... .......... 17% 78.5M 0s\n  7200K .......... .......... .......... .......... .......... 17%  243M 0s\n  7250K .......... .......... .......... .......... .......... 17%  188M 0s\n  7300K .......... .......... .......... .......... .......... 17%  225M 0s\n  7350K .......... .......... .......... .......... .......... 18% 86.1M 0s\n  7400K .......... .......... .......... .......... .......... 18%  227M 0s\n  7450K .......... .......... .......... .......... .......... 18%  225M 0s\n  7500K .......... .......... .......... .......... .......... 18%  167M 0s\n  7550K .......... .......... .......... .......... .......... 18% 75.7M 0s\n  7600K .......... .......... .......... .......... .......... 18%  250M 0s\n  7650K .......... .......... .......... .......... .......... 18%  175M 0s\n  7700K .......... .......... .......... .......... .......... 18%  157M 0s\n  7750K .......... .......... .......... .......... .......... 19%  226M 0s\n  7800K .......... .......... .......... .......... .......... 19% 59.3M 0s\n  7850K .......... .......... .......... .......... .......... 19%  169M 0s\n  7900K .......... .......... .......... .......... .......... 19%  239M 0s\n  7950K .......... .......... .......... .......... .......... 19%  166M 0s\n  8000K .......... .......... .......... .......... .......... 19%  252M 0s\n  8050K .......... .......... .......... .......... .......... 19%  249M 0s\n  8100K .......... .......... .......... .......... .......... 19%  187M 0s\n  8150K .......... .......... .......... .......... .......... 20%  177M 0s\n  8200K .......... .......... .......... .......... .......... 20% 95.3M 0s\n  8250K .......... .......... .......... .......... .......... 20%  251M 0s\n  8300K .......... .......... .......... .......... .......... 20%  233M 0s\n  8350K .......... .......... .......... .......... .......... 20% 11.3M 0s\n  8400K .......... .......... .......... .......... .......... 20%  125M 0s\n  8450K .......... .......... .......... .......... .......... 20%  259M 0s\n  8500K .......... .......... .......... .......... .......... 20%  252M 0s\n  8550K .......... .......... .......... .......... .......... 21%  204M 0s\n  8600K .......... .......... .......... .......... .......... 21%  149M 0s\n  8650K .......... .......... .......... .......... .......... 21%  136M 0s\n  8700K .......... .......... .......... .......... .......... 21%  242M 0s\n  8750K .......... .......... .......... .......... .......... 21%  211M 0s\n  8800K .......... .......... .......... .......... .......... 21%  144M 0s\n  8850K .......... .......... .......... .......... .......... 21%  103M 0s\n  8900K .......... .......... .......... .......... .......... 21%  246M 0s\n  8950K .......... .......... .......... .......... .......... 22%  217M 0s\n  9000K .......... .......... .......... .......... .......... 22%  160M 0s\n  9050K .......... .......... .......... .......... .......... 22%  116M 0s\n  9100K .......... .......... .......... .......... .......... 22%  114M 0s\n  9150K .......... .......... .......... .......... .......... 22%  184M 0s\n  9200K .......... .......... .......... .......... .......... 22%  148M 0s\n  9250K .......... .......... .......... .......... .......... 22% 48.2M 0s\n  9300K .......... .......... .......... .......... .......... 22%  172M 0s\n  9350K .......... .......... .......... .......... .......... 22%  189M 0s\n  9400K .......... .......... .......... .......... .......... 23%  253M 0s\n  9450K .......... .......... .......... .......... .......... 23%  245M 0s\n  9500K .......... .......... .......... .......... .......... 23%  247M 0s\n  9550K .......... .......... .......... .......... .......... 23% 77.5M 0s\n  9600K .......... .......... .......... .......... .......... 23%  179M 0s\n  9650K .......... .......... .......... .......... .......... 23%  251M 0s\n  9700K .......... .......... .......... .......... .......... 23% 78.9M 0s\n  9750K .......... .......... .......... .......... .......... 23%  147M 0s\n  9800K .......... .......... .......... .......... .......... 24%  153M 0s\n  9850K .......... .......... .......... .......... .......... 24%  133M 0s\n  9900K .......... .......... .......... .......... .......... 24%  247M 0s\n  9950K .......... .......... .......... .......... .......... 24%  205M 0s\n 10000K .......... .......... .......... .......... .......... 24%  160M 0s\n 10050K .......... .......... .......... .......... .......... 24%  169M 0s\n 10100K .......... .......... .......... .......... .......... 24%  153M 0s\n 10150K .......... .......... .......... .......... .......... 24%  223M 0s\n 10200K .......... .......... .......... .......... .......... 25%  195M 0s\n 10250K .......... .......... .......... .......... .......... 25% 75.7M 0s\n 10300K .......... .......... .......... .......... .......... 25%  239M 0s\n 10350K .......... .......... .......... .......... .......... 25%  203M 0s\n 10400K .......... .......... .......... .......... .......... 25%  112M 0s\n 10450K .......... .......... .......... .......... .......... 25% 82.8M 0s\n 10500K .......... .......... .......... .......... .......... 25%  252M 0s\n 10550K .......... .......... .......... .......... .......... 25%  208M 0s\n 10600K .......... .......... .......... .......... .......... 26% 65.5M 0s\n 10650K .......... .......... .......... .......... .......... 26%  179M 0s\n 10700K .......... .......... .......... .......... .......... 26%  242M 0s\n 10750K .......... .......... .......... .......... .......... 26% 69.7M 0s\n 10800K .......... .......... .......... .......... .......... 26%  240M 0s\n 10850K .......... .......... .......... .......... .......... 26%  220M 0s\n 10900K .......... .......... .......... .......... .......... 26%  165M 0s\n 10950K .......... .......... .......... .......... .......... 26% 85.2M 0s\n 11000K .......... .......... .......... .......... .......... 27%  238M 0s\n 11050K .......... .......... .......... .......... .......... 27%  227M 0s\n 11100K .......... .......... .......... .......... .......... 27%  139M 0s\n 11150K .......... .......... .......... .......... .......... 27%  173M 0s\n 11200K .......... .......... .......... .......... .......... 27% 89.7M 0s\n 11250K .......... .......... .......... .......... .......... 27%  229M 0s\n 11300K .......... .......... .......... .......... .......... 27%  127M 0s\n 11350K .......... .......... .......... .......... .......... 27% 73.2M 0s\n 11400K .......... .......... .......... .......... .......... 27%  245M 0s\n 11450K .......... .......... .......... .......... .......... 28%  160M 0s\n 11500K .......... .......... .......... .......... .......... 28%  183M 0s\n 11550K .......... .......... .......... .......... .......... 28%  135M 0s\n 11600K .......... .......... .......... .......... .......... 28%  248M 0s\n 11650K .......... .......... .......... .......... .......... 28% 75.9M 0s\n 11700K .......... .......... .......... .......... .......... 28%  208M 0s\n 11750K .......... .......... .......... .......... .......... 28%  196M 0s\n 11800K .......... .......... .......... .......... .......... 28% 16.7M 0s\n 11850K .......... .......... .......... .......... .......... 29%  195M 0s\n 11900K .......... .......... .......... .......... .......... 29%  235M 0s\n 11950K .......... .......... .......... .......... .......... 29%  205M 0s\n 12000K .......... .......... .......... .......... .......... 29%  230M 0s\n 12050K .......... .......... .......... .......... .......... 29%  162M 0s\n 12100K .......... .......... .......... .......... .......... 29%  194M 0s\n 12150K .......... .......... .......... .......... .......... 29%  146M 0s\n 12200K .......... .......... .......... .......... .......... 29%  148M 0s\n 12250K .......... .......... .......... .......... .......... 30%  154M 0s\n 12300K .......... .......... .......... .......... .......... 30%  165M 0s\n 12350K .......... .......... .......... .......... .......... 30%  190M 0s\n 12400K .......... .......... .......... .......... .......... 30%  129M 0s\n 12450K .......... .......... .......... .......... .......... 30% 92.6M 0s\n 12500K .......... .......... .......... .......... .......... 30%  233M 0s\n 12550K .......... .......... .......... .......... .......... 30%  110M 0s\n 12600K .......... .......... .......... .......... .......... 30%  202M 0s\n 12650K .......... .......... .......... .......... .......... 31%  249M 0s\n 12700K .......... .......... .......... .......... .......... 31% 98.4M 0s\n 12750K .......... .......... .......... .......... .......... 31%  219M 0s\n 12800K .......... .......... .......... .......... .......... 31%  110M 0s\n 12850K .......... .......... .......... .......... .......... 31%  181M 0s\n 12900K .......... .......... .......... .......... .......... 31%  250M 0s\n 12950K .......... .......... .......... .......... .......... 31%  109M 0s\n 13000K .......... .......... .......... .......... .......... 31%  232M 0s\n 13050K .......... .......... .......... .......... .......... 32%  112M 0s\n 13100K .......... .......... .......... .......... .......... 32%  251M 0s\n 13150K .......... .......... .......... .......... .......... 32%  215M 0s\n 13200K .......... .......... .......... .......... .......... 32%  122M 0s\n 13250K .......... .......... .......... .......... .......... 32%  224M 0s\n 13300K .......... .......... .......... .......... .......... 32%  114M 0s\n 13350K .......... .......... .......... .......... .......... 32%  224M 0s\n 13400K .......... .......... .......... .......... .......... 32%  222M 0s\n 13450K .......... .......... .......... .......... .......... 33%  103M 0s\n 13500K .......... .......... .......... .......... .......... 33%  169M 0s\n 13550K .......... .......... .......... .......... .......... 33%  123M 0s\n 13600K .......... .......... .......... .......... .......... 33%  165M 0s\n 13650K .......... .......... .......... .......... .......... 33%  230M 0s\n 13700K .......... .......... .......... .......... .......... 33%  139M 0s\n 13750K .......... .......... .......... .......... .......... 33%  140M 0s\n 13800K .......... .......... .......... .......... .......... 33%  221M 0s\n 13850K .......... .......... .......... .......... .......... 33%  101M 0s\n 13900K .......... .......... .......... .......... .......... 34%  190M 0s\n 13950K .......... .......... .......... .......... .......... 34%  132M 0s\n 14000K .......... .......... .......... .......... .......... 34%  126M 0s\n 14050K .......... .......... .......... .......... .......... 34% 93.2M 0s\n 14100K .......... .......... .......... .......... .......... 34%  132M 0s\n 14150K .......... .......... .......... .......... .......... 34%  225M 0s\n 14200K .......... .......... .......... .......... .......... 34%  125M 0s\n 14250K .......... .......... .......... .......... .......... 34%  232M 0s\n 14300K .......... .......... .......... .......... .......... 35%  176M 0s\n 14350K .......... .......... .......... .......... .......... 35%  138M 0s\n 14400K .......... .......... .......... .......... .......... 35%  216M 0s\n 14450K .......... .......... .......... .......... .......... 35%  130M 0s\n 14500K .......... .......... .......... .......... .......... 35%  224M 0s\n 14550K .......... .......... .......... .......... .......... 35%  155M 0s\n 14600K .......... .......... .......... .......... .......... 35%  144M 0s\n 14650K .......... .......... .......... .......... .......... 35%  148M 0s\n 14700K .......... .......... .......... .......... .......... 36%  137M 0s\n 14750K .......... .......... .......... .......... .......... 36%  167M 0s\n 14800K .......... .......... .......... .......... .......... 36%  193M 0s\n 14850K .......... .......... .......... .......... .......... 36%  246M 0s\n 14900K .......... .......... .......... .......... .......... 36%  169M 0s\n 14950K .......... .......... .......... .......... .......... 36% 85.4M 0s\n 15000K .......... .......... .......... .......... .......... 36%  230M 0s\n 15050K .......... .......... .......... .......... .......... 36%  145M 0s\n 15100K .......... .......... .......... .......... .......... 37% 77.7M 0s\n 15150K .......... .......... .......... .......... .......... 37%  164M 0s\n 15200K .......... .......... .......... .......... .......... 37%  245M 0s\n 15250K .......... .......... .......... .......... .......... 37%  157M 0s\n 15300K .......... .......... .......... .......... .......... 37%  200M 0s\n 15350K .......... .......... .......... .......... .......... 37% 92.8M 0s\n 15400K .......... .......... .......... .......... .......... 37%  193M 0s\n 15450K .......... .......... .......... .......... .......... 37%  247M 0s\n 15500K .......... .......... .......... .......... .......... 38%  125M 0s\n 15550K .......... .......... .......... .......... .......... 38%  108M 0s\n 15600K .......... .......... .......... .......... .......... 38%  106M 0s\n 15650K .......... .......... .......... .......... .......... 38%  246M 0s\n 15700K .......... .......... .......... .......... .......... 38%  153M 0s\n 15750K .......... .......... .......... .......... .......... 38%  128M 0s\n 15800K .......... .......... .......... .......... .......... 38%  189M 0s\n 15850K .......... .......... .......... .......... .......... 38%  142M 0s\n 15900K .......... .......... .......... .......... .......... 38%  203M 0s\n 15950K .......... .......... .......... .......... .......... 39%  138M 0s\n 16000K .......... .......... .......... .......... .......... 39% 84.8M 0s\n 16050K .......... .......... .......... .......... .......... 39%  250M 0s\n 16100K .......... .\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n....... 48%  118M 0s\n 15600K .......... .......... .......... .......... .......... 49%  209M 0s\n 15650K .......... .......... .......... .......... .......... 49%  238M 0s\n 15700K .......... .......... .......... .......... .......... 49%  129M 0s\n 15750K .......... .......... .......... .......... .......... 49%  209M 0s\n 15800K .......... .......... .......... .......... .......... 49%  237M 0s\n 15850K .......... .......... .......... .......... .......... 49%  219M 0s\n 15900K .......... .......... .......... .......... .......... 49%  147M 0s\n 15950K .......... .......... .......... .......... .......... 50%  199M 0s\n 16000K .......... .......... .......... .......... .......... 50%  202M 0s\n 16050K .......... .......... .......... .......... .......... 50%  140M 0s\n 16100K .......... .......... .......... .......... .......... 50%  225M 0s\n 16150K .......... .......... .......... .......... .......... 50%  222M 0s\n 16200K .......... .......... .......... .......... .......... 50%  201M 0s\n 16250K .......... .......... .......... .......... .......... 51%  149M 0s\n 16300K .......... .......... .......... .......... .......... 51%  244M 0s\n 16350K .......... .......... .......... .......... .......... 51%  184M 0s\n 16400K .......... .......... .......... .......... .......... 51%  238M 0s\n 16450K .......... .......... .......... .......... .......... 51%  151M 0s\n 16500K .......... .......... .......... .......... .......... 51%  235M 0s\n 16550K .......... .......... .......... .......... .......... 51%  202M 0s\n 16600K .......... .......... .......... .......... .......... 52%  159M 0s\n 16650K .......... .......... .......... .......... .......... 52%  240M 0s\n 16700K .......... .......... .......... .......... .......... 52%  246M 0s\n 16750K .......... .......... .......... .......... .......... 52%  176M 0s\n 16800K .......... .......... .......... .......... .......... 52%  145M 0s\n 16850K .......... .......... .......... .......... .......... 52%  247M 0s\n 16900K .......... .......... .......... .......... .......... 53%  213M 0s\n 16950K .......... .......... .......... .......... .......... 53%  211M 0s\n 17000K .......... .......... .......... .......... .......... 53%  127M 0s\n 17050K .......... .......... .......... .......... .......... 53%  236M 0s\n 17100K .......... .......... .......... .......... .......... 53%  160M 0s\n 17150K .......... .......... .......... .......... .......... 53%  121M 0s\n 17200K .......... .......... .......... .......... .......... 54%  212M 0s\n 17250K .......... .......... .......... .......... .......... 54%  227M 0s\n 17300K .......... .......... .......... .......... .......... 54%  230M 0s\n 17350K .......... .......... .......... .......... .......... 54%  140M 0s\n 17400K .......... .......... .......... .......... .......... 54%  245M 0s\n 17450K .......... .......... .......... .......... .......... 54%  222M 0s\n 17500K .......... .......... .......... .......... .......... 54%  219M 0s\n 17550K .......... .......... .......... .......... .......... 55%  127M 0s\n 17600K .......... .......... .......... .......... .......... 55%  224M 0s\n 17650K .......... .......... .......... .......... .......... 55%  238M 0s\n 17700K .......... .......... .......... .......... .......... 55%  140M 0s\n 17750K .......... .......... .......... .......... .......... 55%  192M 0s\n 17800K .......... .......... .......... .......... .......... 55%  250M 0s\n 17850K .......... .......... .......... .......... .......... 56%  233M 0s\n 17900K .......... .......... .......... .......... .......... 56%  145M 0s\n 17950K .......... .......... .......... .......... .......... 56%  161M 0s\n 18000K .......... .......... .......... .......... .......... 56%  175M 0s\n 18050K .......... .......... .......... .......... .......... 56%  238M 0s\n 18100K .......... .......... .......... .......... .......... 56%  135M 0s\n 18150K .......... .......... .......... .......... .......... 56%  221M 0s\n 18200K .......... .......... .......... .......... .......... 57%  244M 0s\n 18250K .......... .......... .......... .......... .......... 57%  148M 0s\n 18300K .......... .......... .......... .......... .......... 57%  251M 0s\n 18350K .......... .......... .......... .......... .......... 57%  197M 0s\n 18400K .......... .......... .......... .......... .......... 57%  242M 0s\n 18450K .......... .......... .......... .......... .......... 57%  140M 0s\n 18500K .......... .......... .......... .......... .......... 58%  214M 0s\n 18550K .......... .......... .......... .......... .......... 58%  219M 0s\n 18600K .......... .......... .......... .......... .......... 58%  152M 0s\n 18650K .......... .......... .......... .......... .......... 58%  214M 0s\n 18700K .......... .......... .......... .......... .......... 58%  245M 0s\n 18750K .......... .......... .......... .......... .......... 58%  183M 0s\n 18800K .......... .......... .......... .......... .......... 59%  145M 0s\n 18850K .......... .......... .......... .......... .......... 59%  212M 0s\n 18900K .......... .......... .......... .......... .......... 59%  230M 0s\n 18950K .......... .......... .......... .......... .......... 59%  203M 0s\n 19000K .......... .......... .......... .......... .......... 59%  148M 0s\n 19050K .......... .......... .......... .......... .......... 59%  169M 0s\n 19100K .......... .......... .......... .......... .......... 59%  235M 0s\n 19150K .......... .......... .......... .......... .......... 60%  128M 0s\n 19200K .......... .......... .......... .......... .......... 60%  228M 0s\n 19250K .......... .......... .......... .......... .......... 60%  250M 0s\n 19300K .......... .......... .......... .......... .......... 60%  216M 0s\n 19350K .......... .......... .......... .......... .......... 60%  148M 0s\n 19400K .......... .......... .......... .......... .......... 60%  213M 0s\n 19450K .......... .......... .......... .......... .......... 61%  250M 0s\n 19500K .......... .......... .......... .......... .......... 61%  246M 0s\n 19550K .......... .......... .......... .......... .......... 61%  135M 0s\n 19600K .......... .......... .......... .......... .......... 61%  230M 0s\n 19650K .......... .......... .......... .......... .......... 61%  254M 0s\n 19700K .......... .......... .......... .......... .......... 61%  238M 0s\n 19750K .......... .......... .......... .......... .......... 61%  155M 0s\n 19800K .......... .......... .......... .......... .......... 62%  209M 0s\n 19850K .......... .......... .......... .......... .......... 62%  246M 0s\n 19900K .......... .......... .......... .......... .......... 62%  147M 0s\n 19950K .......... .......... .......... .......... .......... 62%  196M 0s\n 20000K .......... .......... .......... .......... .......... 62%  251M 0s\n 20050K .......... .......... .......... .......... .......... 62%  241M 0s\n 20100K .......... .......... .......... .......... .......... 63%  138M 0s\n 20150K .......... .......... .......... .......... .......... 63%  203M 0s\n 20200K .......... .......... .......... .......... .......... 63%  220M 0s\n 20250K .......... .......... .......... .......... .......... 63%  183M 0s\n 20300K .......... .......... .......... .......... .......... 63%  145M 0s\n 20350K .......... .......... .......... .......... .......... 63%  198M 0s\n 20400K .......... .......... .......... .......... .......... 64%  231M 0s\n 20450K .......... .......... .......... .......... .......... 64%  149M 0s\n 20500K .......... .......... .......... .......... .......... 64%  232M 0s\n 20550K .......... .......... .......... .......... .......... 64%  226M 0s\n 20600K .......... .......... .......... .......... .......... 64%  232M 0s\n 20650K .......... .......... .......... .......... .......... 64%  155M 0s\n 20700K .......... .......... .......... .......... .......... 64%  248M 0s\n 20750K .......... .......... .......... .......... .......... 65%  187M 0s\n 20800K .......... .......... .......... .......... .......... 65%  250M 0s\n 20850K .......... .......... .......... .......... .......... 65%  149M 0s\n 20900K .......... .......... .......... .......... .......... 65%  155M 0s\n 20950K .......... .......... .......... .......... .......... 65%  198M 0s\n 21000K .......... .......... .......... .......... .......... 65%  104M 0s\n 21050K .......... .......... .......... .......... .......... 66%  178M 0s\n 21100K .......... .......... .......... .......... .......... 66%  240M 0s\n 21150K .......... .......... .......... .......... .......... 66%  183M 0s\n 21200K .......... .......... .......... .......... .......... 66%  140M 0s\n 21250K .......... .......... .......... .......... .......... 66%  207M 0s\n 21300K .......... .......... .......... .......... .......... 66%  243M 0s\n 21350K .......... .......... .......... .......... .......... 67%  139M 0s\n 21400K .......... .......... .......... .......... .......... 67%  196M 0s\n 21450K .......... .......... .......... .......... .......... 67%  201M 0s\n 21500K .......... .......... .......... .......... .......... 67%  245M 0s\n 21550K .......... .......... .......... .......... .......... 67%  117M 0s\n 21600K .......... .......... .......... .......... .......... 67%  241M 0s\n 21650K .......... .......... .......... .......... .......... 67%  190M 0s\n 21700K .......... .......... .......... .......... .......... 68%  245M 0s\n 21750K .......... .......... .......... .......... .......... 68%  143M 0s\n 21800K .......... .......... .......... .......... .......... 68%  222M 0s\n 21850K .......... .......... .......... .......... .......... 68%  211M 0s\n 21900K .......... .......... .......... .......... .......... 68%  143M 0s\n 21950K .......... .......... .......... .......... .......... 68%  189M 0s\n 22000K .......... .......... .......... .......... .......... 69%  246M 0s\n 22050K .......... .......... .......... .......... .......... 69%  245M 0s\n 22100K .......... .......... .......... .......... .......... 69%  141M 0s\n 22150K .......... .......... .......... .......... .......... 69%  192M 0s\n 22200K .......... .......... .......... .......... .......... 69%  243M 0s\n 22250K .......... .......... .......... .......... .......... 69%  246M 0s\n 22300K .......... .......... .......... .......... .......... 69%  134M 0s\n 22350K .......... .......... .......... .......... .......... 70%  183M 0s\n 22400K .......... .......... .......... .......... .......... 70%  248M 0s\n 22450K .......... .......... .......... .......... .......... 70% 62.2M 0s\n 22500K .......... .......... .......... .......... .......... 70% 64.4M 0s\n 22550K .......... .......... .......... .......... .......... 70% 47.0M 0s\n 22600K .......... .......... .......... .......... .......... 70%  135M 0s\n 22650K .......... .......... .......... .......... .......... 71%  202M 0s\n 22700K .......... .......... .......... .......... .......... 71%  216M 0s\n 22750K .......... .......... .......... .......... .......... 71%  148M 0s\n 22800K .......... .......... .......... .......... .......... 71%  129M 0s\n 22850K .......... .......... .......... .......... .......... 71%  210M 0s\n 22900K .......... .......... .......... .......... .......... 71%  245M 0s\n 22950K .......... .......... .......... .......... .......... 72%  161M 0s\n 23000K .......... .......... .......... .......... .......... 72%  121M 0s\n 23050K .......... .......... .......... .......... .......... 72%  231M 0s\n 23100K .......... .......... .......... .......... .......... 72%  227M 0s\n 23150K .......... .......... .......... .......... .......... 72%  155M 0s\n 23200K .......... .......... .......... .......... .......... 72%  132M 0s\n 23250K .......... .......... .......... .......... .......... 72%  231M 0s\n 23300K .......... .......... .......... .......... .......... 73%  203M 0s\n 23350K .......... .......... .......... .......... .......... 73% 88.4M 0s\n 23400K .......... .......... .......... .......... .......... 73%  232M 0s\n 23450K .......... .......... .......... .......... .......... 73%  205M 0s\n 23500K .......... .......... .......... .......... .......... 73%  131M 0s\n 23550K .......... .......... .......... .......... .......... 73%  129M 0s\n 23600K .......... .......... .......... .......... .......... 74%  186M 0s\n 23650K .......... .......... .......... .......... .......... 74%  198M 0s\n 23700K .......... .......... .......... .......... .......... 74% 72.3M 0s\n 23750K .......... .......... .......... .......... .......... 74%  203M 0s\n 23800K .......... .......... .......... .......... .......... 74%  183M 0s\n 23850K .......... .......... .......... .......... .......... 74%  231M 0s\n 23900K .......... .......... .......... .......... .......... 74%  133M 0s\n 23950K .......... .......... .......... .......... .......... 75%  161M 0s\n 24000K .......... .......... .......... .......... .......... 75%  191M 0s\n 24050K .......... .......... .......... .......... .......... 75%  229M 0s\n 24100K .......... .......... .......... .......... .......... 75%  224M 0s\n 24150K .......... .......... .......... .......... .......... 75%  205M 0s\n 24200K .......... .......... .......... .......... .......... 75%  215M 0s\n 24250K .......... .......... .......... .......... .......... 76%  239M 0s\n 24300K .......... .......... .......... .......... .......... 76%  152M 0s\n 24350K .......... .......... .......... .......... .......... 76%  143M 0s\n 24400K .......... .......... .......... .......... .......... 76%  170M 0s\n 24450K .......... .......... .......... .......... .......... 76%  142M 0s\n 24500K .......... .......... .......... .......... .......... 76%  239M 0s\n 24550K .......... .......... .......... .......... .......... 77%  226M 0s\n 24600K .......... .......... .......... .......... .......... 77%  170M 0s\n 24650K .......... .......... .......... .......... .......... 77%  128M 0s\n 24700K .......... .......... .......... .......... .......... 77%  241M 0s\n 24750K .......... .......... .......... .......... .......... 77%  201M 0s\n 24800K .......... .......... .......... .......... .......... 77%  193M 0s\n 24850K .......... .......... .......... .......... .......... 77%  145M 0s\n 24900K .......... .......... .......... .......... .......... 78%  247M 0s\n 24950K .......... .......... .......... .......... .......... 78%  193M 0s\n 25000K .......... .......... .......... .......... .......... 78%  138M 0s\n 25050K .......... .......... .......... .......... .......... 78%  224M 0s\n 25100K .......... .......... .......... .......... .......... 78%  245M 0s\n 25150K .......... .......... .......... .......... .......... 78%  203M 0s\n 25200K .......... .......... .......... .......... .......... 79%  123M 0s\n 25250K .......... .......... .......... .......... .......... 79%  218M 0s\n 25300K .......... .......... .......... .......... .......... 79%  245M 0s\n 25350K .......... .......... .......... .......... .......... 79%  171M 0s\n 25400K .......... .......... .......... .......... .......... 79%  138M 0s\n 25450K .......... .......... .......... .......... .......... 79%  221M 0s\n 25500K .......... .......... .......... .......... .......... 79%  237M 0s\n 25550K .......... .......... .......... .......... .......... 80%  197M 0s\n 25600K .......... .......... .......... .......... .......... 80%  136M 0s\n 25650K .......... .......... .......... .......... .......... 80%  224M 0s\n 25700K .......... .......... .......... .......... .......... 80%  244M 0s\n 25750K .......... .......... .......... .......... .......... 80%  103M 0s\n 25800K .......... .......... .......... .......... .......... 80%  223M 0s\n 25850K .......... .......... .......... .......... .......... 81%  247M 0s\n 25900K .......... .......... .......... .......... .......... 81%  250M 0s\n 25950K .......... .......... .......... .......... .......... 81%  136M 0s\n 26000K .......... .......... .......... .......... .......... 81%  179M 0s\n 26050K .......... .......... .......... .......... .......... 81%  237M 0s\n 26100K .......... .......... .......... .......... .......... 81%  244M 0s\n 26150K .......... .......... .......... .......... .......... 82%  138M 0s\n 26200K .......... .......... .......... .......... .......... 82%  216M 0s\n 26250K .......... .......... .......... .......... .......... 82%  179M 0s\n 26300K .......... .......... .......... .......... .......... 82%  187M 0s\n 26350K .......... .......... .......... .......... .......... 82%  115M 0s\n 26400K .......... .......... .......... .......... .......... 82%  249M 0s\n 26450K .......... .......... .......... .......... .......... 82%  155M 0s\n 26500K .......... .......... .......... .......... .......... 83%  133M 0s\n 26550K .......... .......... .......... .......... .......... 83%  166M 0s\n 26600K .......... .......... .......... .......... .......... 83%  219M 0s\n 26650K .......... .......... .......... .......... .......... 83%  245M 0s\n 26700K .......... .......... .......... .......... .......... 83%  147M 0s\n 26750K .......... .......... .......... .......... .......... 83%  179M 0s\n 26800K .......... .......... .......... .......... .......... 84%  223M 0s\n 26850K .......... .......... .......... .......... .......... 84%  243M 0s\n 26900K .......... .......... .......... .......... .......... 84%  146M 0s\n 26950K .......... .......... .......... .......... .......... 84%  200M 0s\n 27000K .......... .......... .......... .......... .......... 84%  222M 0s\n 27050K .......... .......... .......... .......... .......... 84%  138M 0s\n 27100K .......... .......... .......... .......... .......... 85%  226M 0s\n 27150K .......... .......... .......... .......... .......... 85%  185M 0s\n 27200K .......... .......... .......... .......... .......... 85%  216M 0s\n 27250K .......... .......... .......... .......... .......... 85%  146M 0s\n 27300K .......... .......... .......... .......... .......... 85%  238M 0s\n 27350K .......... .......... .......... .......... .......... 85%  149M 0s\n 27400K .......... .......... .......... .......... .......... 85%  240M 0s\n 27450K .......... .......... .......... .......... .......... 86%  144M 0s\n 27500K .......... .......... .......... .......... .......... 86%  222M 0s\n 27550K .......... .......... .......... .......... .......... 86%  161M 0s\n 27600K .......... .......... .......... .......... .......... 86% 99.6M 0s\n 27650K .......... .......... .......... .......... .......... 86%  215M 0s\n 27700K .......... .......... .......... .......... .......... 86%  200M 0s\n 27750K .......... .......... .......... .......... .......... 87%  179M 0s\n 27800K .......... .......... .......... .......... .......... 87%  138M 0s\n 27850K .......... .......... .......... .......... .......... 87%  250M 0s\n 27900K .......... .......... .......... .......... .......... 87%  257M 0s\n 27950K .......... .......... .......... .......... .......... 87%  182M 0s\n 28000K .......... .......... .......... .......... .......... 87%  152M 0s\n 28050K .......... .......... .......... .......... .......... 87%  236M 0s\n 28100K .......... .......... .......... .......... .......... 88%  149M 0s\n 28150K .......... .......... .......... .......... .......... 88%  185M 0s\n 28200K .......... .......... .......... .......... .......... 88%  154M 0s\n 28250K .......... .......... .......... .......... .......... 88%  251M 0s\n 28300K .......... .......... .......... .......... .......... 88%  250M 0s\n 28350K .......... .......... .......... .......... .......... 88%  116M 0s\n 28400K .......... .......... .......... .......... .......... 89%  246M 0s\n 28450K .......... .......... .......... .......... .......... 89%  253M 0s\n 28500K .......... .......... .......... .......... .......... 89%  248M 0s\n 28550K .......... .......... .......... .......... .......... 89%  103M 0s\n 28600K .......... .......... .......... .......... .......... 89%  241M 0s\n 28650K .......... .......... .......... .......... .......... 89%  251M 0s\n 28700K .......... .......... .......... .......... .......... 90%  248M 0s\n 28750K .......... .......... .......... .......... .......... 90%  102M 0s\n 28800K .......... .......... .......... .......... .......... 90%  246M 0s\n 28850K .......... .......... .......... .......... .......... 90%  245M 0s\n 28900K .......... .......... .......... .......... .......... 90%  235M 0s\n 28950K .......... .......... .......... .......... .......... 90%  109M 0s\n 29000K .......... .......... .......... .......... .......... 90%  243M 0s\n 29050K .......... .......... .......... .......... .......... 91%  249M 0s\n 29100K .......... .......... .......... .......... .......... 91%  110M 0s\n 29150K .......... .......... .......... .......... .......... 91%  203M 0s\n 29200K .......... .......... .......... .......... .......... 91%  256M 0s\n 29250K .......... .......... .......... .......... .......... 91%  237M 0s\n 29300K .......... .......... .......... .......... .......... 91% 91.8M 0s\n 29350K .......... .......... .......... .......... .......... 92%  161M 0s\n 29400K .......... .......... .......... .......... .......... 92% 42.7M 0s\n 29450K .......... .......... .......... .......... .......... 92%  225M 0s\n 29500K .......... .......... .......... .......... .......... 92% 91.2M 0s\n 29550K .......... .......... .......... .......... .......... 92%  208M 0s\n 29600K .......... .......... .......... .......... .......... 92%  240M 0s\n 29650K .......... .......... .......... .......... .......... 92%  135M 0s\n 29700K .......... .......... .......... .......... .......... 93%  220M 0s\n 29750K .......... .......... .......... .......... .......... 93%  168M 0s\n 29800K .......... .......... .......... .......... .......... 93%  222M 0s\n 29850K .......... .......... .......... .......... .......... 93% 93.7M 0s\n 29900K .......... .......... .......... .......... .......... 93%  254M 0s\n 29950K .......... .......... .......... .......... .......... 93%  208M 0s\n 30000K .......... .......... .......... .......... .......... 94%  243M 0s\n 30050K .......... .......... .......... .......... .......... 94%  104M 0s\n 30100K .......... .......... .......... .......... .......... 94%  234M 0s\n 30150K .......... .......... .......... .......... .......... 94%  233M 0s\n 30200K .......... .......... .......... .......... .......... 94% 64.2M 0s\n 30250K .......... .......... .......... .......... .......... 94%  218M 0s\n 30300K .......... .......... .......... .......... .......... 95%  250M 0s\n 30350K .......... .......... .......... .......... .......... 95%  215M 0s\n 30400K .......... .......... .......... .......... .......... 95% 82.0M 0s\n 30450K .......... .......... .......... .......... .......... 95%  250M 0s\n 30500K .......... .......... .......... .......... .......... 95%  249M 0s\n 30550K .......... .......... .......... .......... .......... 95%  219M 0s\n 30600K .......... .......... .......... .......... .......... 95%  140M 0s\n 30650K .......... .......... .......... .......... .......... 96%  244M 0s\n 30700K .......... .......... .......... .......... .......... 96%  243M 0s\n 30750K .......... .......... .......... .......... .......... 96%  206M 0s\n 30800K .......... .......... .......... .......... .......... 96%  147M 0s\n 30850K .......... .......... .......... .......... .......... 96%  246M 0s\n 30900K .......... .......... .......... .......... .......... 96%  245M 0s\n 30950K .......... .......... .......... .......... .......... 97%  145M 0s\n 31000K .......... .......... .......... .......... .......... 97%  247M 0s\n 31050K .......... .......... .......... .......... .......... 97%  221M 0s\n 31100K .......... .......... .......... .......... .......... 97%  246M 0s\n 31150K .......... .......... .......... .......... .......... 97% 74.0M 0s\n 31200K .......... .......... .......... .......... .......... 97%  211M 0s\n 31250K .......... .......... .......... .......... .......... 98% 89.0M 0s\n 31300K .......... .......... .......... .......... .......... 98%  246M 0s\n 31350K .......... .......... .......... .......... .......... 98%  194M 0s\n 31400K .......... .......... .......... .......... .......... 98%  239M 0s\n 31450K .......... .......... .......... .......... .......... 98%  149M 0s\n 31500K .......... .......... .......... .......... .......... 98%  178M 0s\n 31550K .......... .......... .......... .......... .......... 98%  174M 0s\n 31600K .......... .......... .......... .......... .......... 99%  243M 0s\n 31650K .......... .......... .......... .......... .......... 99%  149M 0s\n 31700K .......... .......... .......... .......... .......... 99%  247M 0s\n 31750K .......... .......... .......... .......... .......... 99%  200M 0s\n 31800K .......... .......... .......... .......... .......... 99%  239M 0s\n 31850K .......... .......... .......... .......... .......... 99%  154M 0s\n 31900K .......... .......... .......... .......              100%  193M=0.2s\n\n2023-08-17 05:43:00 (162 MB/s) - ‘/tmp/externals/cuda/libcusolver-dev-11-7_11.4.0.1-1_amd64.deb’ saved [32704192/32704192]\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package libcurand-dev-11-7.\n(Reading database ... 98324 files and directories currently installed.)\nPreparing to unpack .../libcurand-dev-11-7_10.2.10.50-1_amd64.deb ...\nUnpacking libcurand-dev-11-7 (10.2.10.50-1) ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dpkg: dependency problems prevent configuration of libcurand-dev-11-7:\n libcurand-dev-11-7 depends on libcurand-11-7 (>= 10.2.10.50); however:\n  Package libcurand-11-7 is not installed.\n\ndpkg: error processing package libcurand-dev-11-7 (--install):\n dependency problems - leaving unconfigured\nErrors were encountered while processing:\n libcurand-dev-11-7\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package libcusparse-dev-11-7.\n(Reading database ... 98357 files and directories currently installed.)\nPreparing to unpack .../libcusparse-dev-11-7_11.7.3.50-1_amd64.deb ...\nUnpacking libcusparse-dev-11-7 (11.7.3.50-1) ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dpkg: dependency problems prevent configuration of libcusparse-dev-11-7:\n libcusparse-dev-11-7 depends on libcusparse-11-7 (>= 11.7.3.50); however:\n  Package libcusparse-11-7 is not installed.\n\ndpkg: error processing package libcusparse-dev-11-7 (--install):\n dependency problems - leaving unconfigured\nErrors were encountered while processing:\n libcusparse-dev-11-7\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package libcublas-dev-11-7.\n(Reading database ... 98370 files and directories currently installed.)\nPreparing to unpack .../libcublas-dev-11-7_11.10.1.25-1_amd64.deb ...\nUnpacking libcublas-dev-11-7 (11.10.1.25-1) ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dpkg: dependency problems prevent configuration of libcublas-dev-11-7:\n libcublas-dev-11-7 depends on libcublas-11-7 (>= 11.10.1.25); however:\n  Package libcublas-11-7 is not installed.\n\ndpkg: error processing package libcublas-dev-11-7 (--install):\n dependency problems - leaving unconfigured\nErrors were encountered while processing:\n libcublas-dev-11-7\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package libcusolver-dev-11-7.\n(Reading database ... 98392 files and directories currently installed.)\nPreparing to unpack .../libcusolver-dev-11-7_11.4.0.1-1_amd64.deb ...\nUnpacking libcusolver-dev-11-7 (11.4.0.1-1) ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dpkg: dependency problems prevent configuration of libcusolver-dev-11-7:\n libcusolver-dev-11-7 depends on libcusolver-11-7 (>= 11.4.0.1); however:\n  Package libcusolver-11-7 is not installed.\n\ndpkg: error processing package libcusolver-dev-11-7 (--install):\n dependency problems - leaving unconfigured\nErrors were encountered while processing:\n libcusolver-dev-11-7\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "mkdir -p /tmp/externals/cuda\n",
    "\n",
    "wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcurand-dev-11-7_10.2.10.50-1_amd64.deb -P /tmp/externals/cuda\n",
    "wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusparse-dev-11-7_11.7.3.50-1_amd64.deb -P /tmp/externals/cuda\n",
    "wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcublas-dev-11-7_11.10.1.25-1_amd64.deb -P /tmp/externals/cuda\n",
    "wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusolver-dev-11-7_11.4.0.1-1_amd64.deb -P /tmp/externals/cuda\n",
    "\n",
    "dpkg -i /tmp/externals/cuda/libcurand-dev-11-7_10.2.10.50-1_amd64.deb\n",
    "dpkg -i /tmp/externals/cuda/libcusparse-dev-11-7_11.7.3.50-1_amd64.deb\n",
    "dpkg -i /tmp/externals/cuda/libcublas-dev-11-7_11.10.1.25-1_amd64.deb\n",
    "dpkg -i /tmp/externals/cuda/libcusolver-dev-11-7_11.4.0.1-1_amd64.deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c107709a-786a-46d2-bca7-094d503a2b87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting deepspeed==0.9.1\n  Downloading deepspeed-0.9.1.tar.gz (766 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 766.2/766.2 kB 9.7 MB/s eta 0:00:00\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting py-cpuinfo==9.0.0\n  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\nCollecting hjson\n  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 5.7 MB/s eta 0:00:00\nCollecting ninja\n  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 146.0/146.0 kB 17.7 MB/s eta 0:00:00\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.10/site-packages (from deepspeed==0.9.1) (1.21.5)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.10/site-packages (from deepspeed==0.9.1) (21.3)\nRequirement already satisfied: psutil in /databricks/python3/lib/python3.10/site-packages (from deepspeed==0.9.1) (5.9.0)\nRequirement already satisfied: pydantic<2.0.0 in /databricks/python3/lib/python3.10/site-packages (from deepspeed==0.9.1) (1.10.6)\nRequirement already satisfied: torch in /databricks/python3/lib/python3.10/site-packages (from deepspeed==0.9.1) (1.13.1+cpu)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.10/site-packages (from deepspeed==0.9.1) (4.64.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from packaging>=20.0->deepspeed==0.9.1) (3.0.9)\nRequirement already satisfied: typing-extensions>=4.2.0 in /databricks/python3/lib/python3.10/site-packages (from pydantic<2.0.0->deepspeed==0.9.1) (4.3.0)\nBuilding wheels for collected packages: deepspeed\n  Building wheel for deepspeed (setup.py): started\n  Building wheel for deepspeed (setup.py): finished with status 'done'\n  Created wheel for deepspeed: filename=deepspeed-0.9.1-py3-none-any.whl size=798545 sha256=3ec5e1fba383870bb8c146f9a719c5c81d7aa216d261c6bcdbbd8818123028bf\n  Stored in directory: /root/.cache/pip/wheels/22/fb/58/11dd2b0b2490eaaea3708f4115ea6cdb702de5beccb512b478\nSuccessfully built deepspeed\nInstalling collected packages: py-cpuinfo, ninja, hjson, deepspeed\nSuccessfully installed deepspeed-0.9.1 hjson-3.1.0 ninja-1.11.1 py-cpuinfo-9.0.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install deepspeed==0.9.1 py-cpuinfo==9.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c47c4b00-0a3b-447f-a51a-1ba004344dfd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| enumerating serving endpoints...found 0...(0 seconds)\n| removing the working directory \"dbfs:/mnt/dbacademy-users/vijaymohire@bhadaleit.onmicrosoft.com/large-language-models\"...(0 seconds)\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/large-language-models/v01\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing lab testing framework.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nUsing the \"default\" schema.\n\nPredefined paths variables:\n| DA.paths.working_dir: /dbfs/mnt/dbacademy-users/vijaymohire@bhadaleit.onmicrosoft.com/large-language-models\n| DA.paths.user_db:     /dbfs/mnt/dbacademy-users/vijaymohire@bhadaleit.onmicrosoft.com/large-language-models/database.db\n| DA.paths.datasets:    /dbfs/mnt/dbacademy-datasets/large-language-models/v01\n\nSetup completed (13 seconds)\n\nThe models developed or used in this course are for demonstration and learning purposes only.\nModels may occasionally output offensive, inaccurate, biased information, or harmful instructions.\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "335fea72-1dda-405e-af3d-64e372c8a041",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username:          vijaymohire\nWorking Directory: /dbfs/mnt/dbacademy-users/vijaymohire@bhadaleit.onmicrosoft.com/large-language-models\n"
     ]
    }
   ],
   "source": [
    "print(f\"Username:          vijaymohire\")\n",
    "print(f\"Working Directory: /dbfs/mnt/dbacademy-users/vijaymohire@bhadaleit.onmicrosoft.com/large-language-models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dcf5219-278b-4654-a445-7e2d423d47ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfb26147-69e7-4bc0-8c88-ec0d5b385564",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Creating a local temporary directory on the Driver. This will serve as a root directory for the intermediate model checkpoints created during the training process. The final model will be persisted to DBFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5119758d-3ec0-428b-a513-52be21d6f2cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "tmpdir = tempfile.TemporaryDirectory()\n",
    "local_training_root = tmpdir.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "061df562-3fa8-44ac-bfef-28e6677ac74b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b290af9-5bdd-4115-960f-4ebbb33d270c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import transformers as tr\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec8e4517-285e-4c56-8693-d964476c5bf6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 1 - Data Preparation\n",
    "\n",
    "The first step of the fine-tuning process is to identify a specific task and supporting dataset. In this notebook, we will consider the specific task to be classifying movie reviews. This idea is generally simple task where a movie review is provided as plain-text and we would like to determine whether or not the review was positive or negative.\n",
    "\n",
    "The [IMDB dataset](https://huggingface.co/datasets/imdb) can be leveraged as a supporting dataset for this task. The dataset conveniently provides both a training and testing dataset with labeled binary sentiments, as well as a dataset of unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83318bde-5ed1-47db-8ed9-1d22b9bbaf4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:27: UserWarning: This dataset can not be stored in DBFS because either `cache_dir` or the environment variable `HF_DATASETS_CACHE` is set to a non-DBFS path. If this cluster restarts, all saved dataset information will be lost.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919b57e4d7504181acd73550ece2e8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49b0c9c65bd45d0bf5b906a3fe65792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df19688811a64bd4ade48bf4584e5614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:13: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imdb/plain_text to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cacf21c5d2a045a291eea32a1a451558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc91e7f84d2e4e99932c285ed275e4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89e1c1b992f4a208f5012f818c7ed59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8be8baba0af45fbbd3a943aaedfbedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2fb463bb0574522b4f4bf3fa151b608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb_ds = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "177a0b50-66fb-42c3-91ad-99ddaeedd59f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 2 - Select pre-trained model\n",
    "\n",
    "The next step of the fine-tuning process is to select a pre-trained model. We will consider using the [T5](https://huggingface.co/docs/transformers/model_doc/t5) [[paper]](https://arxiv.org/pdf/1910.10683.pdf) family of models for our fine-tuning purposes. The T5 models are text-to-text transformers that have been trained on a multi-task mixture of unsupervised and supervised tasks. They are well suited for tasks such as summarization, translation, text classification, question answering, and more.\n",
    "\n",
    "The `t5-small` version of the T5 models has 60 million parameters. This slimmed down version will be sufficient for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "994ccabd-c773-4d73-8bc3-f33e18161481",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"t5-small\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26e55a62-8a9b-4c0e-baa9-5ec4b739c2fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Recall from Module 1, Hugging Face provides the [Auto*](https://huggingface.co/docs/transformers/model_doc/auto) suite of objects to conveniently instantiate the various components associated with a pre-trained model. Here, we use the [AutoTokenizer](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer) to load in the tokenizer that is associated with the `t5-small` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8fd5e0-b64c-454c-a152-34b961333a3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the tokenizer that was used for the t5-small model\n",
    "tokenizer = tr.AutoTokenizer.from_pretrained(\n",
    "    model_checkpoint, cache_dir=\"/dbfs/mnt/dbacademy-datasets/large-language-models/v01\"\n",
    ")  # Use a pre-cached model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b74aa87-06d4-4bb2-971f-0981e7f5c65a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As mentioned above, the IMDB dataset is a binary sentiment dataset. Its labels therefore are encoded as (-1 - unknown; 0 - negative; 1 - positive) values. In order to use this dataset with a text-to-text model like T5, the label set needs to be represented as a string. There are a number of ways to accomplish this. Here, we will simply translate each label id to its corresponding string value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83a9eb90-eb8e-4c56-91d1-1ff8d7189c19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_tokens(\n",
    "    tokenizer: tr.models.t5.tokenization_t5_fast.T5TokenizerFast, label_map: dict\n",
    ") -> callable:\n",
    "    \"\"\"\n",
    "    Given a `tokenizer` this closure will iterate through `x` and return the result of `apply()`.\n",
    "    This function is mapped to a dataset and returned with ids and attention mask.\n",
    "    \"\"\"\n",
    "\n",
    "    def apply(x) -> tr.tokenization_utils_base.BatchEncoding:\n",
    "        \"\"\"From a formatted dataset `x` a batch encoding `token_res` is created.\"\"\"\n",
    "        target_labels = [label_map[y] for y in x[\"label\"]]\n",
    "        token_res = tokenizer(\n",
    "            x[\"text\"],\n",
    "            text_target=target_labels,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "        )\n",
    "        return token_res\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "imdb_label_lookup = {0: \"negative\", 1: \"positive\", -1: \"unknown\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4cf368b-103a-449d-ad51-b890e817a4ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709b17fec6fe4e4faa0ee27b98ad593f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b3ec0c576642e9bebff0effc8979dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18b49a3f3a1498cba71862681936f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb_to_tokens = to_tokens(tokenizer, imdb_label_lookup)\n",
    "tokenized_dataset = imdb_ds.map(\n",
    "    imdb_to_tokens, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9906c4ab-b671-411f-8710-44e32c8978a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 3 - Setup Training\n",
    "\n",
    "The model training process is highly configurable. The [TrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) class effectively exposes the configurable aspects of the process allowing one to customize them accordingly. Here, we will focus on setting up a training process that performs a single epoch of training with a batch size of 16. We will also leverage `adamw_torch` as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e83dab1-8c95-4750-9776-572c26a54337",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+torch_distributed_hint": "",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1488117105151061>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m checkpoint_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest-trainer\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m      2\u001B[0m local_checkpoint_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(local_training_root, checkpoint_name)\n",
       "\u001B[0;32m----> 3\u001B[0m training_args \u001B[38;5;241m=\u001B[39m tr\u001B[38;5;241m.\u001B[39mTrainingArguments(\n",
       "\u001B[1;32m      4\u001B[0m     local_checkpoint_path,\n",
       "\u001B[1;32m      5\u001B[0m     num_train_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,  \u001B[38;5;66;03m# default number of epochs to train is 3\u001B[39;00m\n",
       "\u001B[1;32m      6\u001B[0m     per_device_train_batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m,\n",
       "\u001B[1;32m      7\u001B[0m     optim\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madamw_torch\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      8\u001B[0m     report_to\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtensorboard\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n",
       "\u001B[1;32m      9\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m<string>:110\u001B[0m, in \u001B[0;36m__init__\u001B[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, xpu_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode)\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/training_args.py:1259\u001B[0m, in \u001B[0;36mTrainingArguments.__post_init__\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1253\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m version\u001B[38;5;241m.\u001B[39mparse(version\u001B[38;5;241m.\u001B[39mparse(torch\u001B[38;5;241m.\u001B[39m__version__)\u001B[38;5;241m.\u001B[39mbase_version) \u001B[38;5;241m==\u001B[39m version\u001B[38;5;241m.\u001B[39mparse(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2.0.0\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16:\n",
       "\u001B[1;32m   1254\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1256\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n",
       "\u001B[1;32m   1257\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1258\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m is_torch_available()\n",
       "\u001B[0;32m-> 1259\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1260\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (get_xla_device_type(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGPU\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1261\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16 \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16_full_eval)\n",
       "\u001B[1;32m   1262\u001B[0m ):\n",
       "\u001B[1;32m   1263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m   1264\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1265\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (`--fp16_full_eval`) can only be used on CUDA devices.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1266\u001B[0m     )\n",
       "\u001B[1;32m   1268\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n",
       "\u001B[1;32m   1269\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1270\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m is_torch_available()\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1275\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16 \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16_full_eval)\n",
       "\u001B[1;32m   1276\u001B[0m ):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/training_args.py:1694\u001B[0m, in \u001B[0;36mTrainingArguments.device\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1690\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1691\u001B[0m \u001B[38;5;124;03mThe device used by this process.\u001B[39;00m\n",
       "\u001B[1;32m   1692\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1693\u001B[0m requires_backends(\u001B[38;5;28mself\u001B[39m, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
       "\u001B[0;32m-> 1694\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_devices\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/utils/generic.py:54\u001B[0m, in \u001B[0;36mcached_property.__get__\u001B[0;34m(self, obj, objtype)\u001B[0m\n",
       "\u001B[1;32m     52\u001B[0m cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(obj, attr, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
       "\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cached \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m---> 54\u001B[0m     cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     55\u001B[0m     \u001B[38;5;28msetattr\u001B[39m(obj, attr, cached)\n",
       "\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cached\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/training_args.py:1679\u001B[0m, in \u001B[0;36mTrainingArguments._setup_devices\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1677\u001B[0m         torch\u001B[38;5;241m.\u001B[39mdistributed\u001B[38;5;241m.\u001B[39minit_process_group(backend\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mxpu_backend, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mddp_timeout_delta)\n",
       "\u001B[1;32m   1678\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1679\u001B[0m         \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistributed\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit_process_group\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbackend\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnccl\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mddp_timeout_delta\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1680\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlocal_rank)\n",
       "\u001B[1;32m   1681\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_gpu \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:761\u001B[0m, in \u001B[0;36minit_process_group\u001B[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)\u001B[0m\n",
       "\u001B[1;32m    757\u001B[0m         \u001B[38;5;66;03m# Use a PrefixStore to avoid accidental overrides of keys used by\u001B[39;00m\n",
       "\u001B[1;32m    758\u001B[0m         \u001B[38;5;66;03m# different systems (e.g. RPC) in case the store is multi-tenant.\u001B[39;00m\n",
       "\u001B[1;32m    759\u001B[0m         store \u001B[38;5;241m=\u001B[39m PrefixStore(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdefault_pg\u001B[39m\u001B[38;5;124m\"\u001B[39m, store)\n",
       "\u001B[0;32m--> 761\u001B[0m     default_pg \u001B[38;5;241m=\u001B[39m \u001B[43m_new_process_group_helper\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m    762\u001B[0m \u001B[43m        \u001B[49m\u001B[43mworld_size\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    763\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    764\u001B[0m \u001B[43m        \u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    765\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbackend\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    766\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    767\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpg_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpg_options\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    768\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgroup_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup_name\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    769\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    770\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    771\u001B[0m     _update_default_pg(default_pg)\n",
       "\u001B[1;32m    773\u001B[0m _pg_group_ranks[GroupMember\u001B[38;5;241m.\u001B[39mWORLD] \u001B[38;5;241m=\u001B[39m {i: i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(GroupMember\u001B[38;5;241m.\u001B[39mWORLD\u001B[38;5;241m.\u001B[39msize())}  \u001B[38;5;66;03m# type: ignore[attr-defined, index]\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:886\u001B[0m, in \u001B[0;36m_new_process_group_helper\u001B[0;34m(group_size, group_rank, global_ranks_in_group, backend, store, pg_options, group_name, timeout)\u001B[0m\n",
       "\u001B[1;32m    884\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m backend \u001B[38;5;241m==\u001B[39m Backend\u001B[38;5;241m.\u001B[39mNCCL:\n",
       "\u001B[1;32m    885\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_nccl_available():\n",
       "\u001B[0;32m--> 886\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDistributed package doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt have NCCL \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuilt in\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    887\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m pg_options \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    888\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n",
       "\u001B[1;32m    889\u001B[0m             pg_options, ProcessGroupNCCL\u001B[38;5;241m.\u001B[39mOptions\n",
       "\u001B[1;32m    890\u001B[0m         ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected pg_options argument to be of type ProcessGroupNCCL.Options\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\n",
       "\u001B[0;31mRuntimeError\u001B[0m: Distributed package doesn't have NCCL built in"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)\nFile \u001B[0;32m<command-1488117105151061>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m checkpoint_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest-trainer\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      2\u001B[0m local_checkpoint_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(local_training_root, checkpoint_name)\n\u001B[0;32m----> 3\u001B[0m training_args \u001B[38;5;241m=\u001B[39m tr\u001B[38;5;241m.\u001B[39mTrainingArguments(\n\u001B[1;32m      4\u001B[0m     local_checkpoint_path,\n\u001B[1;32m      5\u001B[0m     num_train_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,  \u001B[38;5;66;03m# default number of epochs to train is 3\u001B[39;00m\n\u001B[1;32m      6\u001B[0m     per_device_train_batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m,\n\u001B[1;32m      7\u001B[0m     optim\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madamw_torch\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      8\u001B[0m     report_to\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtensorboard\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m      9\u001B[0m )\n\nFile \u001B[0;32m<string>:110\u001B[0m, in \u001B[0;36m__init__\u001B[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, xpu_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode)\u001B[0m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/training_args.py:1259\u001B[0m, in \u001B[0;36mTrainingArguments.__post_init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1253\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m version\u001B[38;5;241m.\u001B[39mparse(version\u001B[38;5;241m.\u001B[39mparse(torch\u001B[38;5;241m.\u001B[39m__version__)\u001B[38;5;241m.\u001B[39mbase_version) \u001B[38;5;241m==\u001B[39m version\u001B[38;5;241m.\u001B[39mparse(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2.0.0\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16:\n\u001B[1;32m   1254\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1256\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1257\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1258\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m is_torch_available()\n\u001B[0;32m-> 1259\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1260\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (get_xla_device_type(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGPU\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1261\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16 \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16_full_eval)\n\u001B[1;32m   1262\u001B[0m ):\n\u001B[1;32m   1263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1264\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1265\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (`--fp16_full_eval`) can only be used on CUDA devices.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1266\u001B[0m     )\n\u001B[1;32m   1268\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1269\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1270\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m is_torch_available()\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1275\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16 \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16_full_eval)\n\u001B[1;32m   1276\u001B[0m ):\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/training_args.py:1694\u001B[0m, in \u001B[0;36mTrainingArguments.device\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1690\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1691\u001B[0m \u001B[38;5;124;03mThe device used by this process.\u001B[39;00m\n\u001B[1;32m   1692\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1693\u001B[0m requires_backends(\u001B[38;5;28mself\u001B[39m, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m-> 1694\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_devices\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/utils/generic.py:54\u001B[0m, in \u001B[0;36mcached_property.__get__\u001B[0;34m(self, obj, objtype)\u001B[0m\n\u001B[1;32m     52\u001B[0m cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(obj, attr, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cached \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 54\u001B[0m     cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28msetattr\u001B[39m(obj, attr, cached)\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cached\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/training_args.py:1679\u001B[0m, in \u001B[0;36mTrainingArguments._setup_devices\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1677\u001B[0m         torch\u001B[38;5;241m.\u001B[39mdistributed\u001B[38;5;241m.\u001B[39minit_process_group(backend\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mxpu_backend, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mddp_timeout_delta)\n\u001B[1;32m   1678\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1679\u001B[0m         \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistributed\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit_process_group\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbackend\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnccl\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mddp_timeout_delta\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1680\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlocal_rank)\n\u001B[1;32m   1681\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_gpu \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:761\u001B[0m, in \u001B[0;36minit_process_group\u001B[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)\u001B[0m\n\u001B[1;32m    757\u001B[0m         \u001B[38;5;66;03m# Use a PrefixStore to avoid accidental overrides of keys used by\u001B[39;00m\n\u001B[1;32m    758\u001B[0m         \u001B[38;5;66;03m# different systems (e.g. RPC) in case the store is multi-tenant.\u001B[39;00m\n\u001B[1;32m    759\u001B[0m         store \u001B[38;5;241m=\u001B[39m PrefixStore(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdefault_pg\u001B[39m\u001B[38;5;124m\"\u001B[39m, store)\n\u001B[0;32m--> 761\u001B[0m     default_pg \u001B[38;5;241m=\u001B[39m \u001B[43m_new_process_group_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    762\u001B[0m \u001B[43m        \u001B[49m\u001B[43mworld_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    763\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    764\u001B[0m \u001B[43m        \u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    765\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbackend\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    766\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    767\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpg_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpg_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    768\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgroup_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    769\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    770\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    771\u001B[0m     _update_default_pg(default_pg)\n\u001B[1;32m    773\u001B[0m _pg_group_ranks[GroupMember\u001B[38;5;241m.\u001B[39mWORLD] \u001B[38;5;241m=\u001B[39m {i: i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(GroupMember\u001B[38;5;241m.\u001B[39mWORLD\u001B[38;5;241m.\u001B[39msize())}  \u001B[38;5;66;03m# type: ignore[attr-defined, index]\u001B[39;00m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:886\u001B[0m, in \u001B[0;36m_new_process_group_helper\u001B[0;34m(group_size, group_rank, global_ranks_in_group, backend, store, pg_options, group_name, timeout)\u001B[0m\n\u001B[1;32m    884\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m backend \u001B[38;5;241m==\u001B[39m Backend\u001B[38;5;241m.\u001B[39mNCCL:\n\u001B[1;32m    885\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_nccl_available():\n\u001B[0;32m--> 886\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDistributed package doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt have NCCL \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuilt in\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    887\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m pg_options \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    888\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[1;32m    889\u001B[0m             pg_options, ProcessGroupNCCL\u001B[38;5;241m.\u001B[39mOptions\n\u001B[1;32m    890\u001B[0m         ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected pg_options argument to be of type ProcessGroupNCCL.Options\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\n\u001B[0;31mRuntimeError\u001B[0m: Distributed package doesn't have NCCL built in",
       "errorSummary": "<span class='ansi-red-fg'>RuntimeError</span>: Distributed package doesn't have NCCL built in",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint_name = \"test-trainer\"\n",
    "local_checkpoint_path = os.path.join(local_training_root, checkpoint_name)\n",
    "training_args = tr.TrainingArguments(\n",
    "    local_checkpoint_path,\n",
    "    num_train_epochs=1,  # default number of epochs to train is 3\n",
    "    per_device_train_batch_size=16,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=[\"tensorboard\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9269057a-25cc-4574-95a3-de013e42c16c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The pre-trained `t5-small` model can be loaded using the [AutoModelForSeq2SeqLM](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSeq2SeqLM) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2084171a-9f03-438f-91d6-e092f63b8d42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the pre-trained model\n",
    "model = tr.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_checkpoint, cache_dir=DA.paths.datasets\n",
    ")  # Use a pre-cached model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30b45fb1-61fd-4ae9-adf2-fee3eaf98dee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# used to assist the trainer in batching the data\n",
    "data_collator = tr.DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = tr.Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6669f56-f17b-4001-a0ea-acb6c5c89f9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 4 - Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0809fa7-0368-415e-a9cc-68b7898a636f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Before starting the training process, let's turn on Tensorboard. This will allow us to monitor the training process as checkpoint logs are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89594ab5-31b4-4a8c-8f31-31ffa9ba0694",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tensorboard_display_dir = f\"{local_checkpoint_path}/runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db4f551e-7088-42b7-9575-1e52ded4ef1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir '{tensorboard_display_dir}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32f6206c-1faa-4222-82fc-a75dd265138d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Start the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32f6a61d-57bd-402c-910f-6721254e1b5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "# save model to the local checkpoint\n",
    "trainer.save_model()\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec182f54-d483-48f7-b2a0-5cf2118c1d99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# persist the fine-tuned model to DBFS\n",
    "final_model_path = f\"{DA.paths.working_dir}/llm04_fine_tuning/{checkpoint_name}\"\n",
    "trainer.save_model(output_dir=final_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82adbef6-325d-457f-b93f-5229354b57da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 5 - Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13fe7ca7-5a81-4fb0-a082-e8bcdcc940ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fine_tuned_model = tr.AutoModelForSeq2SeqLM.from_pretrained(final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f11af3b1-75ed-4104-8a9f-1812a2497efd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"\"\"\n",
    "'Despicable Me' is a cute and funny movie, but the plot is predictable and the characters are not very well-developed. Overall, it's a good movie for kids, but adults might find it a bit boring.\"\"\",\n",
    "    \"\"\" 'The Batman' is a dark and gritty take on the Caped Crusader, starring Robert Pattinson as Bruce Wayne. The film is a well-made crime thriller with strong performances and visuals, but it may be too slow-paced and violent for some viewers.\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "The Phantom Menace is a visually stunning film with some great action sequences, but the plot is slow-paced and the dialogue is often wooden. It is a mixed bag that will appeal to some fans of the Star Wars franchise, but may disappoint others.\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "I'm not sure if The Matrix and the two sequels were meant to have a tigh consistency but I don't think they quite fit together. They seem to have a reasonably solid arc but the features from the first aren't in the second and third as much, instead the second and third focus more on CGI battles and more visuals. I like them but for different reasons, so if I'm supposed to rate the trilogy I'm not sure what to say.\n",
    "\"\"\",\n",
    "]\n",
    "inputs = tokenizer(reviews, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "pred = fine_tuned_model.generate(\n",
    "    input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "beb7474a-9831-41a9-b05d-f882c6fe8066",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame(\n",
    "    zip(reviews, tokenizer.batch_decode(pred, skip_special_tokens=True)),\n",
    "    columns=[\"review\", \"classification\"],\n",
    ")\n",
    "display(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d783534b-b1ef-4141-9acf-f025c3a9cbc4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DeepSpeed\n",
    "\n",
    "As model architectures evolve and grow, they continually push the limits of available computational resources. For example, some large LLMs having hundreds of billions of parameters making them too large to fit, in some cases, in available GPU memory. Models of this scale therefore need to leverage distributed processing or high-end hardware, and sometimes even both, to support training efforts. This makes large model training a costly undertaking, and therefore accelerating the training process is highly desirable.\n",
    "\n",
    "As mentioned above, one such framework that can be leveraged to accelerate the model training process is Microsoft's [DeepSpeed](https://github.com/microsoft/DeepSpeed) [[paper]](https://arxiv.org/pdf/2207.00032.pdf). This framework provides advances in compression, distributed training, mixed precision, gradient accumulation, and checkpointing.\n",
    "\n",
    "It is worth noting that DeepSpeed is intended for large models that do not fit into device memory. The `t5-base` model we are using is not a large model, and therefore DeepSpeed is not expected to provide a benefit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2d773ae-f9bb-4cd2-8f41-b2596f27dc24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Environment Setup\n",
    "\n",
    "The intended use for DeepSpeed is in a distributed compute environment. As such, each node of the environment is assigned a `rank` and `local_rank` in relation to the size of the distributed environment.\n",
    "\n",
    "Here, since we are testing with a single node/GPU environment we will set the `world_size` to 1, and both `ranks` to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaea8388-2d10-4fe7-a318-59da284afd3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"9994\"  # modify if RuntimeError: Address already in use\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7224823b-191a-4b7c-b0be-3f2838103597",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Configuration\n",
    "\n",
    "There are a number of [configuration options](https://www.deepspeed.ai/docs/config-json/) that can be set to enhance the training and inference processes. The [ZeRO optimization](https://www.deepspeed.ai/training/#memory-efficiency) settings target reducing the memory footprint allowing for larger models to be efficiently trained on limited resources. \n",
    "\n",
    "The Hugging Face `TrainerArguments` accept the configuration either from a JSON file or a dictionary. Here, we will define the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fce8e31-d01b-416b-9f22-bdd036e24291",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "zero_config = {\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"offload_optimizer\": {\"device\": \"cpu\", \"pin_memory\": True},\n",
    "        \"allgather_partitions\": True,\n",
    "        \"allgather_bucket_size\": 5e8,\n",
    "        \"overlap_comm\": True,\n",
    "        \"reduce_scatter\": True,\n",
    "        \"reduce_bucket_size\": 5e8,\n",
    "        \"contiguous_gradients\": True,\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": \"auto\",\n",
    "            \"eps\": \"auto\",\n",
    "            \"weight_decay\": \"auto\",\n",
    "            \"torch_adam\": True,\n",
    "        },\n",
    "    },\n",
    "    \"train_batch_size\": \"auto\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56c79c4d-a9f4-4480-84a7-b9d6cc9a054b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"t5-base\"\n",
    "tokenizer = tr.AutoTokenizer.from_pretrained(\n",
    "    model_checkpoint, cache_dir=DA.paths.datasets\n",
    ")\n",
    "\n",
    "imdb_to_tokens = to_tokens(tokenizer, imdb_label_lookup)\n",
    "tokenized_dataset = imdb_ds.map(\n",
    "    imdb_to_tokens, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "\n",
    "model = tr.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_checkpoint, cache_dir=DA.paths.datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "212761c7-c77a-4948-8681-53a192168adc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93d4d41c-cfa9-41d2-821d-a70d1db78703",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are only two changes made to the training setup from above. The first is to set a new checkpoint name. The second is to add the `deepspeed` configuration to the `TrainingArguments`.\n",
    "\n",
    "Note: at this time the `deepspeed` argument is considered an experimental feature and may evolve in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac33e6b4-fc24-4550-8796-bd065694991b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_name = \"test-trainer-deepspeed\"\n",
    "checkpoint_location = os.path.join(local_training_root, checkpoint_name)\n",
    "training_args = tr.TrainingArguments(\n",
    "    checkpoint_location,\n",
    "    num_train_epochs=3,  # default number of epochs to train is 3\n",
    "    per_device_train_batch_size=8,\n",
    "    deepspeed=zero_config,  # add the deepspeed configuration\n",
    "    report_to=[\"tensorboard\"],\n",
    ")\n",
    "\n",
    "data_collator = tr.DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = tr.Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bc75481-88e5-4dd3-afd8-e9bfdf5617cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tensorboard_display_dir = f\"{checkpoint_location}/runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc8e47bf-8faf-4197-9e56-d38ad6e2b62d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir '{tensorboard_display_dir}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "142412a4-c1c3-43e7-8f83-7da605694312",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model()\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "352b3213-94d3-4b99-a246-600d4ce7669f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# persist the fine-tuned model to DBFS\n",
    "final_model_path = f\"{DA.paths.working_dir}/llm04_fine_tuning/{checkpoint_name}\"\n",
    "trainer.save_model(output_dir=final_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ae4dd6d-69e3-4d79-a32f-ff4e846d5bff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "692665e2-473d-4778-b8e6-f905097d7a26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fine_tuned_model = tr.AutoModelForSeq2SeqLM.from_pretrained(final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd20463e-3d40-4847-9aee-f90e5cb99696",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "review = [\n",
    "    \"\"\"\n",
    "           I'm not sure if The Matrix and the two sequels were meant to have a tight consistency but I don't think they quite fit together. They seem to have a reasonably solid arc but the features from the first aren't in the second and third as much, instead the second and third focus more on CGI battles and more visuals. I like them but for different reasons, so if I'm supposed to rate the trilogy I'm not sure what to say.\"\"\"\n",
    "]\n",
    "inputs = tokenizer(review, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "pred = fine_tuned_model.generate(\n",
    "    input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55a985c3-fd8c-4d16-8269-03acb57d7cec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame(\n",
    "    zip(review, tokenizer.batch_decode(pred, skip_special_tokens=True)),\n",
    "    columns=[\"review\", \"classification\"],\n",
    ")\n",
    "display(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54a2488f-daf9-4e14-9ba7-397dbab988bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Clean up Classroom\n",
    "\n",
    "Run the following cell to remove lessons-specific assets created during this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "633e4baa-865a-46d8-b61a-c69726fce3b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tmpdir.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dda5dd0-e872-4e09-8a5e-8a21e8aedd38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1488117105151042,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "LLM 04a - Fine-tuning LLMs",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
